{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHLN6yNzn05mDGa0cVLAjv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BRUTE18/Aerial_Image_Segementation/blob/main/Untitled64.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dQx7cqbt1mkX"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Ola Bike Ride Request Forecast using Machine Learning\n",
        "---------------------------------------------------\n",
        "A comprehensive implementation of the research paper by Anunay Kumar and Utkarsh Raj\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import holidays\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "import xgboost as xgb\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from prophet import Prophet\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Flatten\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class OlaBikeForecast:\n",
        "    \"\"\"\n",
        "    A class to implement the Ola Bike Ride Request Forecasting system\n",
        "    as described in the research paper.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cities=None):\n",
        "        \"\"\"\n",
        "        Initialize the forecasting system.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        cities : list, optional\n",
        "            List of cities to include in the analysis. Default is\n",
        "            [\"Bangalore\", \"Mumbai\", \"Hyderabad\", \"Pune\", \"Delhi\"]\n",
        "        \"\"\"\n",
        "        self.cities = cities or [\"Bangalore\", \"Mumbai\", \"Hyderabad\", \"Pune\", \"Delhi\"]\n",
        "        self.models = {}\n",
        "        self.preprocessors = {}\n",
        "        self.india_holidays = holidays.India()\n",
        "        self.zone_info = None\n",
        "        self.event_data = None\n",
        "        self.traffic_data = None\n",
        "        self.weather_data = None\n",
        "        self.ride_data = None\n",
        "\n",
        "    def load_data(self, ride_data_path, weather_data_path=None,\n",
        "                event_data_path=None, traffic_data_path=None, zone_info_path=None):\n",
        "        \"\"\"\n",
        "        Load all required datasets for the forecasting system.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        ride_data_path : str\n",
        "            Path to the ride request data CSV.\n",
        "        weather_data_path : str, optional\n",
        "            Path to the weather data CSV.\n",
        "        event_data_path : str, optional\n",
        "            Path to the event data CSV.\n",
        "        traffic_data_path : str, optional\n",
        "            Path to the traffic data CSV.\n",
        "        zone_info_path : str, optional\n",
        "            Path to the zone information CSV.\n",
        "        \"\"\"\n",
        "        print(\"Loading ride data...\")\n",
        "        self.ride_data = pd.read_csv(ride_data_path)\n",
        "        self.ride_data['timestamp'] = pd.to_datetime(self.ride_data['timestamp'])\n",
        "\n",
        "        if weather_data_path:\n",
        "            print(\"Loading weather data...\")\n",
        "            self.weather_data = pd.read_csv(weather_data_path)\n",
        "            self.weather_data['timestamp'] = pd.to_datetime(self.weather_data['timestamp'])\n",
        "\n",
        "        if event_data_path:\n",
        "            print(\"Loading event data...\")\n",
        "            self.event_data = pd.read_csv(event_data_path)\n",
        "            self.event_data['event_date'] = pd.to_datetime(self.event_data['event_date'])\n",
        "\n",
        "        if traffic_data_path:\n",
        "            print(\"Loading traffic data...\")\n",
        "            self.traffic_data = pd.read_csv(traffic_data_path)\n",
        "            self.traffic_data['timestamp'] = pd.to_datetime(self.traffic_data['timestamp'])\n",
        "\n",
        "        if zone_info_path:\n",
        "            print(\"Loading zone information...\")\n",
        "            self.zone_info = pd.read_csv(zone_info_path)\n",
        "\n",
        "        print(\"Data loading complete!\")\n",
        "\n",
        "    def generate_dummy_data(self, start_date='2022-01-01', end_date='2023-06-30',\n",
        "                           num_zones=50, seed=42):\n",
        "        \"\"\"\n",
        "        Generate synthetic data for demonstration purposes when real data is not available.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        start_date : str\n",
        "            Start date for the synthetic data.\n",
        "        end_date : str\n",
        "            End date for the synthetic data.\n",
        "        num_zones : int\n",
        "            Number of geographical zones to generate per city.\n",
        "        seed : int\n",
        "            Random seed for reproducibility.\n",
        "        \"\"\"\n",
        "        np.random.seed(seed)\n",
        "        print(\"Generating synthetic data for demonstration...\")\n",
        "\n",
        "        # Create date range\n",
        "        date_range = pd.date_range(start=start_date, end=end_date, freq='30min')\n",
        "\n",
        "        # Generate zone IDs for each city\n",
        "        all_zones = []\n",
        "        for city in self.cities:\n",
        "            for i in range(1, num_zones + 1):\n",
        "                all_zones.append(f\"{city}_zone_{i}\")\n",
        "\n",
        "        # Create empty ride data dataframe\n",
        "        ride_data = []\n",
        "\n",
        "        # Generate synthetic patterns\n",
        "        for zone in all_zones:\n",
        "            city = zone.split('_')[0]\n",
        "            zone_num = int(zone.split('_')[-1])\n",
        "\n",
        "            # Different baseline demand for different cities and zones\n",
        "            base_demand = np.random.randint(10, 40)\n",
        "\n",
        "            # Zone type affects the pattern\n",
        "            zone_type = np.random.choice(['residential', 'commercial', 'industrial', 'transportation_hub'], p=[0.5, 0.3, 0.1, 0.1])\n",
        "\n",
        "            for timestamp in date_range:\n",
        "                hour = timestamp.hour\n",
        "                day_of_week = timestamp.dayofweek\n",
        "                month = timestamp.month\n",
        "\n",
        "                # Base demand for this time\n",
        "                demand = base_demand\n",
        "\n",
        "                # Hour of day effect (rush hours)\n",
        "                if 8 <= hour <= 10:  # Morning rush\n",
        "                    demand *= 1.5\n",
        "                elif 17 <= hour <= 19:  # Evening rush\n",
        "                    demand *= 1.7\n",
        "                elif 0 <= hour <= 5:  # Late night\n",
        "                    demand *= 0.3\n",
        "\n",
        "                # Day of week effect\n",
        "                if day_of_week >= 5:  # Weekend\n",
        "                    if zone_type == 'commercial':\n",
        "                        demand *= 1.2\n",
        "                    elif zone_type == 'residential':\n",
        "                        demand *= 0.8\n",
        "                    elif zone_type == 'industrial':\n",
        "                        demand *= 0.4\n",
        "\n",
        "                # Monthly seasonality\n",
        "                if month in [6, 7, 8]:  # Rainy season\n",
        "                    demand *= 0.85\n",
        "                elif month in [11, 12]:  # Festival season\n",
        "                    demand *= 1.2\n",
        "\n",
        "                # Add some randomness\n",
        "                demand = int(demand * np.random.normal(1, 0.15))\n",
        "                demand = max(0, demand)  # Ensure non-negative\n",
        "\n",
        "                # Create ride entry\n",
        "                ride_data.append({\n",
        "                    'timestamp': timestamp,\n",
        "                    'city': city,\n",
        "                    'zone_id': zone,\n",
        "                    'zone_type': zone_type,\n",
        "                    'ride_requests': demand,\n",
        "                    'completed_rides': int(demand * np.random.uniform(0.8, 0.95)),\n",
        "                    'latitude': np.random.uniform(10, 30),\n",
        "                    'longitude': np.random.uniform(70, 90)\n",
        "                })\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        self.ride_data = pd.DataFrame(ride_data)\n",
        "\n",
        "        # Generate weather data\n",
        "        weather_data = []\n",
        "        for city in self.cities:\n",
        "            for timestamp in pd.date_range(start=start_date, end=end_date, freq='1H'):\n",
        "                month = timestamp.month\n",
        "\n",
        "                # Seasonal temperature variations\n",
        "                if month in [12, 1, 2]:  # Winter\n",
        "                    temp_base = np.random.uniform(10, 20)\n",
        "                elif month in [3, 4, 5]:  # Summer\n",
        "                    temp_base = np.random.uniform(25, 40)\n",
        "                elif month in [6, 7, 8, 9]:  # Monsoon\n",
        "                    temp_base = np.random.uniform(20, 30)\n",
        "                else:  # Autumn\n",
        "                    temp_base = np.random.uniform(18, 28)\n",
        "\n",
        "                # Daily temperature cycle\n",
        "                hour = timestamp.hour\n",
        "                if 0 <= hour <= 6:\n",
        "                    temp_adj = -2\n",
        "                elif 7 <= hour <= 10:\n",
        "                    temp_adj = 0\n",
        "                elif 11 <= hour <= 16:\n",
        "                    temp_adj = 3\n",
        "                else:\n",
        "                    temp_adj = 0\n",
        "\n",
        "                temperature = temp_base + temp_adj\n",
        "\n",
        "                # Precipitation (more likely in monsoon)\n",
        "                if month in [6, 7, 8, 9]:\n",
        "                    precipitation_prob = 0.3\n",
        "                else:\n",
        "                    precipitation_prob = 0.05\n",
        "\n",
        "                precipitation = 0\n",
        "                weather_condition = 'Clear'\n",
        "\n",
        "                if np.random.random() < precipitation_prob:\n",
        "                    precipitation = np.random.exponential(5)\n",
        "                    if precipitation > 10:\n",
        "                        weather_condition = 'Heavy Rain'\n",
        "                    elif precipitation > 2:\n",
        "                        weather_condition = 'Light Rain'\n",
        "                    else:\n",
        "                        weather_condition = 'Drizzle'\n",
        "\n",
        "                # Humidity tends to be higher with precipitation and in monsoon\n",
        "                if precipitation > 0:\n",
        "                    humidity = np.random.uniform(70, 95)\n",
        "                elif month in [6, 7, 8, 9]:\n",
        "                    humidity = np.random.uniform(60, 85)\n",
        "                else:\n",
        "                    humidity = np.random.uniform(30, 70)\n",
        "\n",
        "                weather_data.append({\n",
        "                    'timestamp': timestamp,\n",
        "                    'city': city,\n",
        "                    'temperature': temperature,\n",
        "                    'precipitation': precipitation,\n",
        "                    'humidity': humidity,\n",
        "                    'wind_speed': np.random.uniform(0, 15),\n",
        "                    'weather_condition': weather_condition\n",
        "                })\n",
        "\n",
        "        self.weather_data = pd.DataFrame(weather_data)\n",
        "\n",
        "        # Generate event data\n",
        "        event_data = []\n",
        "        event_types = ['Concert', 'Sports Match', 'Festival', 'Conference', 'Exhibition']\n",
        "        event_sizes = ['Small', 'Medium', 'Large']\n",
        "\n",
        "        # Create ~500 events spread across the date range and cities\n",
        "        for _ in range(500):\n",
        "            event_date = pd.Timestamp(np.random.choice(date_range)).normalize()\n",
        "            duration_hours = np.random.choice([3, 4, 6, 8, 12])\n",
        "            city = np.random.choice(self.cities)\n",
        "            zone_id = f\"{city}_zone_{np.random.randint(1, num_zones + 1)}\"\n",
        "\n",
        "            event_data.append({\n",
        "                'event_date': event_date,\n",
        "                'start_time': pd.Timestamp(event_date) + pd.Timedelta(hours=np.random.randint(8, 20)),\n",
        "                'duration_hours': duration_hours,\n",
        "                'city': city,\n",
        "                'zone_id': zone_id,\n",
        "                'event_type': np.random.choice(event_types),\n",
        "                'event_size': np.random.choice(event_sizes, p=[0.5, 0.3, 0.2]),\n",
        "                'estimated_attendance': np.random.randint(100, 10000)\n",
        "            })\n",
        "\n",
        "        self.event_data = pd.DataFrame(event_data)\n",
        "\n",
        "        # Generate zone information\n",
        "        zone_info = []\n",
        "        for city in self.cities:\n",
        "            for i in range(1, num_zones + 1):\n",
        "                zone_id = f\"{city}_zone_{i}\"\n",
        "                zone_type = np.random.choice(['residential', 'commercial', 'industrial', 'transportation_hub'], p=[0.5, 0.3, 0.1, 0.1])\n",
        "\n",
        "                # Population density varies by zone type\n",
        "                if zone_type == 'residential':\n",
        "                    pop_density = np.random.uniform(5000, 20000)\n",
        "                elif zone_type == 'commercial':\n",
        "                    pop_density = np.random.uniform(1000, 8000)\n",
        "                elif zone_type == 'industrial':\n",
        "                    pop_density = np.random.uniform(500, 3000)\n",
        "                else:  # transportation hub\n",
        "                    pop_density = np.random.uniform(2000, 10000)\n",
        "\n",
        "                zone_info.append({\n",
        "                    'zone_id': zone_id,\n",
        "                    'city': city,\n",
        "                    'zone_type': zone_type,\n",
        "                    'area_sqkm': np.random.uniform(0.8, 1.2),\n",
        "                    'population_density': pop_density,\n",
        "                    'poi_density': np.random.uniform(10, 100),\n",
        "                    'avg_income_level': np.random.choice(['low', 'medium', 'high']),\n",
        "                    'has_metro_station': np.random.choice([True, False], p=[0.3, 0.7]),\n",
        "                    'has_train_station': np.random.choice([True, False], p=[0.2, 0.8]),\n",
        "                    'has_bus_terminal': np.random.choice([True, False], p=[0.4, 0.6]),\n",
        "                    'center_latitude': np.random.uniform(10, 30),\n",
        "                    'center_longitude': np.random.uniform(70, 90)\n",
        "                })\n",
        "\n",
        "        self.zone_info = pd.DataFrame(zone_info)\n",
        "\n",
        "        # Generate traffic data\n",
        "        traffic_data = []\n",
        "        for city in self.cities:\n",
        "            for timestamp in pd.date_range(start=start_date, end=end_date, freq='1H'):\n",
        "                hour = timestamp.hour\n",
        "                day_of_week = timestamp.dayofweek\n",
        "\n",
        "                # Base congestion level varies by time\n",
        "                if 8 <= hour <= 10 or 17 <= hour <= 19:  # Rush hours\n",
        "                    base_congestion = np.random.uniform(0.6, 0.9)\n",
        "                elif 0 <= hour <= 5:  # Late night\n",
        "                    base_congestion = np.random.uniform(0.1, 0.3)\n",
        "                else:\n",
        "                    base_congestion = np.random.uniform(0.3, 0.6)\n",
        "\n",
        "                # Weekday vs weekend\n",
        "                if day_of_week >= 5:  # Weekend\n",
        "                    base_congestion *= 0.7\n",
        "\n",
        "                traffic_data.append({\n",
        "                    'timestamp': timestamp,\n",
        "                    'city': city,\n",
        "                    'congestion_level': base_congestion,\n",
        "                    'avg_speed_kmph': 60 * (1 - base_congestion) + 10  # Speed decreases with congestion\n",
        "                })\n",
        "\n",
        "        self.traffic_data = pd.DataFrame(traffic_data)\n",
        "\n",
        "        print(\"Synthetic data generation complete!\")\n",
        "\n",
        "    def preprocess_data(self, time_interval='30min', test_size=0.2):\n",
        "        \"\"\"\n",
        "        Preprocess the data to create features for forecasting.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        time_interval : str\n",
        "            Time interval for aggregating ride requests.\n",
        "        test_size : float\n",
        "            Fraction of data to use for testing.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict\n",
        "            Dictionary containing training and testing DataFrames for each city.\n",
        "        \"\"\"\n",
        "        print(\"Preprocessing data...\")\n",
        "\n",
        "        # Ensure ride data is sorted by timestamp\n",
        "        self.ride_data = self.ride_data.sort_values(['city', 'zone_id', 'timestamp'])\n",
        "\n",
        "        # Aggregate ride requests at the specified interval\n",
        "        agg_df = self.ride_data.set_index('timestamp').groupby([\n",
        "            pd.Grouper(freq=time_interval), 'city', 'zone_id'\n",
        "        ])['ride_requests'].sum().reset_index()\n",
        "\n",
        "        # Create temporal features\n",
        "        agg_df['hour'] = agg_df['timestamp'].dt.hour\n",
        "        agg_df['day_of_week'] = agg_df['timestamp'].dt.dayofweek\n",
        "        agg_df['day_of_month'] = agg_df['timestamp'].dt.day\n",
        "        agg_df['month'] = agg_df['timestamp'].dt.month\n",
        "        agg_df['year'] = agg_df['timestamp'].dt.year\n",
        "        agg_df['is_weekend'] = agg_df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
        "\n",
        "        # Add holiday indicators\n",
        "        agg_df['is_holiday'] = agg_df['timestamp'].dt.date.apply(\n",
        "            lambda x: 1 if x in self.india_holidays else 0\n",
        "        )\n",
        "\n",
        "        # Merge with zone information if available\n",
        "        if self.zone_info is not None:\n",
        "            agg_df = pd.merge(agg_df, self.zone_info, on=['city', 'zone_id'], how='left')\n",
        "\n",
        "        # Process and merge weather data if available\n",
        "        if self.weather_data is not None:\n",
        "            # Prepare weather data\n",
        "            weather_df = self.weather_data.copy()\n",
        "\n",
        "            # Join weather data to ride data based on nearest timestamp and city\n",
        "            agg_df = pd.merge_asof(\n",
        "                agg_df.sort_values('timestamp'),\n",
        "                weather_df.sort_values('timestamp')[['timestamp', 'city', 'temperature', 'precipitation', 'humidity', 'wind_speed', 'weather_condition']],\n",
        "                on='timestamp',\n",
        "                by='city',\n",
        "                direction='nearest'\n",
        "            )\n",
        "\n",
        "            # One-hot encode weather conditions\n",
        "            agg_df = pd.get_dummies(agg_df, columns=['weather_condition'], prefix='weather')\n",
        "\n",
        "        # Process and merge event data if available\n",
        "        if self.event_data is not None:\n",
        "            # Prepare event features for each timestamp and zone\n",
        "            event_features = []\n",
        "\n",
        "            for _, row in agg_df.iterrows():\n",
        "                city = row['city']\n",
        "                zone_id = row['zone_id']\n",
        "                ts = row['timestamp']\n",
        "\n",
        "                # Find events happening in this zone around this time\n",
        "                relevant_events = self.event_data[\n",
        "                    (self.event_data['city'] == city) &\n",
        "                    (self.event_data['zone_id'] == zone_id) &\n",
        "                    (ts >= self.event_data['start_time']) &\n",
        "                    (ts <= self.event_data['start_time'] + pd.to_timedelta(self.event_data['duration_hours'], unit='h'))\n",
        "                ]\n",
        "\n",
        "                has_event = len(relevant_events) > 0\n",
        "                event_size = 'None'\n",
        "                estimated_attendance = 0\n",
        "\n",
        "                if has_event:\n",
        "                    # If multiple events, take the largest one\n",
        "                    largest_event = relevant_events.loc[relevant_events['estimated_attendance'].idxmax()] if len(relevant_events) > 0 else None\n",
        "                    if largest_event is not None:\n",
        "                        event_size = largest_event['event_size']\n",
        "                        estimated_attendance = largest_event['estimated_attendance']\n",
        "\n",
        "                event_features.append({\n",
        "                    'has_event': int(has_event),\n",
        "                    'event_size_small': 1 if event_size == 'Small' else 0,\n",
        "                    'event_size_medium': 1 if event_size == 'Medium' else 0,\n",
        "                    'event_size_large': 1 if event_size == 'Large' else 0,\n",
        "                    'estimated_attendance': estimated_attendance\n",
        "                })\n",
        "\n",
        "            event_df = pd.DataFrame(event_features)\n",
        "            agg_df = pd.concat([agg_df, event_df], axis=1)\n",
        "\n",
        "        # Process and merge traffic data if available\n",
        "        if self.traffic_data is not None:\n",
        "            # Join traffic data to ride data based on nearest timestamp and city\n",
        "            agg_df = pd.merge_asof(\n",
        "                agg_df.sort_values('timestamp'),\n",
        "                self.traffic_data.sort_values('timestamp')[['timestamp', 'city', 'congestion_level', 'avg_speed_kmph']],\n",
        "                on='timestamp',\n",
        "                by='city',\n",
        "                direction='nearest'\n",
        "            )\n",
        "\n",
        "        # Create lag features (previous hours/days demand)\n",
        "        for lag in [1, 2, 3, 24, 48, 168]:  # 1hr, 2hr, 3hr, 1day, 2day, 1week\n",
        "            agg_df[f'lag_{lag}'] = agg_df.groupby(['city', 'zone_id'])['ride_requests'].shift(lag)\n",
        "\n",
        "        # Create rolling statistics\n",
        "        for window in [3, 24, 168]:  # 3hr, 1day, 1week\n",
        "            # Rolling mean\n",
        "            agg_df[f'rolling_mean_{window}'] = agg_df.groupby(['city', 'zone_id'])['ride_requests'].transform(\n",
        "                lambda x: x.shift(1).rolling(window=window, min_periods=1).mean()\n",
        "            )\n",
        "            # Rolling standard deviation\n",
        "            agg_df[f'rolling_std_{window}'] = agg_df.groupby(['city', 'zone_id'])['ride_requests'].transform(\n",
        "                lambda x: x.shift(1).rolling(window=window, min_periods=1).std()\n",
        "            )\n",
        "\n",
        "        # Drop rows with NaN values created by lagging\n",
        "        agg_df = agg_df.dropna()\n",
        "\n",
        "        # Prepare city-specific datasets\n",
        "        datasets = {}\n",
        "\n",
        "        for city in self.cities:\n",
        "            city_df = agg_df[agg_df['city'] == city].copy()\n",
        "\n",
        "            if len(city_df) == 0:\n",
        "                print(f\"No data available for {city}, skipping...\")\n",
        "                continue\n",
        "\n",
        "            # Determine the split point\n",
        "            split_idx = int(len(city_df) * (1 - test_size))\n",
        "\n",
        "            # Split into training and testing sets (keeping time order)\n",
        "            train_df = city_df.iloc[:split_idx]\n",
        "            test_df = city_df.iloc[split_idx:]\n",
        "\n",
        "            datasets[city] = {\n",
        "                'train': train_df,\n",
        "                'test': test_df,\n",
        "                'all': city_df\n",
        "            }\n",
        "\n",
        "            print(f\"Processed {city}: {len(train_df)} training samples, {len(test_df)} testing samples\")\n",
        "\n",
        "        print(\"Data preprocessing complete!\")\n",
        "        return datasets\n",
        "\n",
        "    def build_feature_pipeline(self, df):\n",
        "        \"\"\"\n",
        "        Build a preprocessing pipeline for the features.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        df : pandas.DataFrame\n",
        "            DataFrame containing the features.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        sklearn.compose.ColumnTransformer\n",
        "            Preprocessing pipeline for transforming features.\n",
        "        \"\"\"\n",
        "        # Identify numeric and categorical columns\n",
        "        numeric_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "        categorical_features = df.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
        "\n",
        "        # Remove target variable and timestamp from features\n",
        "        if 'ride_requests' in numeric_features:\n",
        "            numeric_features.remove('ride_requests')\n",
        "        if 'timestamp' in numeric_features:\n",
        "            numeric_features.remove('timestamp')\n",
        "        if 'timestamp' in categorical_features:\n",
        "            categorical_features.remove('timestamp')\n",
        "        if 'city' in categorical_features:\n",
        "            categorical_features.remove('city')\n",
        "        if 'zone_id' in categorical_features:\n",
        "            categorical_features.remove('zone_id')\n",
        "\n",
        "        # Create preprocessing steps\n",
        "        numeric_transformer = Pipeline(steps=[\n",
        "            ('scaler', StandardScaler())\n",
        "        ])\n",
        "\n",
        "        categorical_transformer = Pipeline(steps=[\n",
        "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "        ])\n",
        "\n",
        "        # Create column transformer\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', numeric_transformer, numeric_features),\n",
        "                ('cat', categorical_transformer, categorical_features)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        return preprocessor\n",
        "\n",
        "    def train_arima_model(self, train_df, zone_id, order=(5,1,0)):\n",
        "        \"\"\"\n",
        "        Train an ARIMA model for a specific zone.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        train_df : pandas.DataFrame\n",
        "            Training data.\n",
        "        zone_id : str\n",
        "            Zone identifier.\n",
        "        order : tuple\n",
        "            ARIMA order parameters (p,d,q).\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        model : ARIMA model\n",
        "            Trained ARIMA model.\n",
        "        \"\"\"\n",
        "        # Filter data for the specific zone\n",
        "        zone_data = train_df[train_df['zone_id'] == zone_id]['ride_requests']\n",
        "\n",
        "        # Train ARIMA model\n",
        "        model = ARIMA(zone_data, order=order)\n",
        "        model_fit = model.fit()\n",
        "\n",
        "        return model_fit\n",
        "\n",
        "    def train_prophet_model(self, train_df, zone_id):\n",
        "        \"\"\"\n",
        "        Train a Prophet model for a specific zone.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        train_df : pandas.DataFrame\n",
        "            Training data.\n",
        "        zone_id : str\n",
        "            Zone identifier.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        model : Prophet model\n",
        "            Trained Prophet model.\n",
        "        \"\"\"\n",
        "        # Filter data for the specific zone\n",
        "        zone_data = train_df[train_df['zone_id'] == zone_id][['timestamp', 'ride_requests']]\n",
        "\n",
        "        # Rename columns for Prophet\n",
        "        prophet_df = zone_data.rename(columns={'timestamp': 'ds', 'ride_requests': 'y'})\n",
        "\n",
        "        # Initialize and train Prophet model\n",
        "        model = Prophet(interval_width=0.95, daily_seasonality=True, weekly_seasonality=True)\n",
        "        model.fit(prophet_df)\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_xgboost_model(self, train_df, preprocessor, target_col='ride_requests'):\n",
        "        \"\"\"\n",
        "        Train an XGBoost model.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        train_df : pandas.DataFrame\n",
        "            Training data.\n",
        "        preprocessor : sklearn.compose.ColumnTransformer\n",
        "            Feature preprocessing pipeline.\n",
        "        target_col : str\n",
        "            Target column name.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        model : XGBoost model\n",
        "            Trained XGBoost model.\n",
        "        \"\"\"\n",
        "        # Prepare the data\n",
        "        X = train_df.drop(['ride_requests', 'timestamp', 'city', 'zone_id'], axis=1)\n",
        "        y = train_df[target_col]\n",
        "\n",
        "        # Preprocess features\n",
        "        X_processed = preprocessor.fit_transform(X)\n",
        "\n",
        "        # Train XGBoost model\n",
        "        model = xgb.XGBRegressor(\n",
        "            n_estimators=200,\n",
        "            learning_rate=0.1,\n",
        "            max_depth=7,\n",
        "            min_child_weight=1,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            objective='reg:squarederror',\n",
        "            random_state=42\n",
        "        )\n",
        "        model.fit(X_processed, y)\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_random_forest_model(self, train_df, preprocessor, target_col='ride_requests'):\n",
        "        \"\"\"\n",
        "        Train a Random Forest model.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        train_df : pandas.DataFrame\n",
        "            Training data.\n",
        "        preprocessor : sklearn.compose.ColumnTransformer\n",
        "            Feature preprocessing pipeline.\n",
        "        target_col : str\n",
        "            Target column name.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        model : RandomForest model\n",
        "            Trained Random Forest model.\n",
        "        \"\"\"\n",
        "        # Prepare the data\n",
        "        X = train_df.drop(['ride_requests', 'timestamp', 'city', 'zone_id'], axis=1)\n",
        "        y = train_df[target_col]\n",
        "\n",
        "        # Preprocess features\n",
        "        X_processed = preprocessor.fit_transform(X)\n",
        "\n",
        "        # Train Random Forest model\n",
        "        model = RandomForestRegressor(\n",
        "            n_estimators=100,\n",
        "            max_depth=15,\n",
        "            min_samples_split=5,\n",
        "            min_samples_leaf=2,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        model.fit(X_processed, y)\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_lstm_model(self, train_df, zone_ids, sequence_length=24):\n",
        "        \"\"\"\n",
        "        Train an LSTM model for time series forecasting.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        train_df : pandas.DataFrame\n",
        "            Training data.\n",
        "        zone_ids : list\n",
        "            List of zone IDs to include in training.\n",
        "        sequence_length : int\n",
        "            Number of time steps to use for each sequence.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        model : Keras LSTM model\n",
        "            Trained LSTM model.\n",
        "        scalers : dict\n",
        "            Dictionary of scalers for each zone.\n",
        "        \"\"\"\n",
        "        # Prepare data for LSTM (we'll use only temporal features and lagged values)\n",
        "        X_sequences = []\n",
        "        y_values = []\n",
        "        scalers = {}\n",
        "\n",
        "        for zone_id in zone_ids:\n",
        "            # Get zone-specific data\n",
        "            zone_data = train_df[train_df['zone_id'] == zone_id].sort_values('timestamp')\n",
        "\n",
        "            # Select features\n",
        "            features = ['hour', 'day_of_week', 'is_weekend', 'is_holiday', 'ride_requests']\n",
        "            weather_features = [col for col in zone_data.columns if col.startswith('weather_')]\n",
        "            features.extend(weather_features)\n",
        "\n",
        "            # Extract feature matrix\n",
        "            data = zone_data[features].values\n",
        "\n",
        "            # Scale the data\n",
        "            scaler = StandardScaler()\n",
        "            scaled_data = scaler.fit_transform(data)\n",
        "            scalers[zone_id] = scaler\n",
        "\n",
        "            # Create sequences\n",
        "            for i in range(len(scaled_data) - sequence_length):\n",
        "                X_sequences.append(scaled_data[i:i+sequence_length, :])\n",
        "                y_values.append(scaled_data[i+sequence_length, -1])  # Predict ride_requests\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        X = np.array(X_sequences)\n",
        "        y = np.array(y_values)\n",
        "\n",
        "        # Build LSTM model\n",
        "        model = Sequential([\n",
        "            LSTM(128, input_shape=(sequence_length, X.shape[2]), return_sequences=True),\n",
        "            Dropout(0.2),\n",
        "            LSTM(64),\n",
        "            Dropout(0.2),\n",
        "            Dense(32, activation='relu'),\n",
        "            Dense(1)\n",
        "        ])\n",
        "\n",
        "        # Compile the model\n",
        "        model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "        # Train the model\n",
        "        model.fit(X, y, epochs=20, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "        return model, scalers\n",
        "\n",
        "    def train_tcn_model(self, train_df, zone_ids, sequence_length=24):\n",
        "        \"\"\"\n",
        "        Train a Temporal Convolutional Network (TCN) model.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        train_df : pandas.DataFrame\n",
        "            Training data.\n",
        "        zone_ids : list\n",
        "            List of zone IDs to include in training.\n",
        "        sequence_length : int\n",
        "            Number of time steps to use for each sequence.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        model : Keras TCN model\n",
        "            Trained TCN model.\n",
        "        scalers : dict\n",
        "            Dictionary of scalers for each zone.\n",
        "        \"\"\"\n",
        "        # Prepare data (similar to LSTM preparation)\n",
        "        X_sequences = []\n",
        "        y_values = []\n",
        "        scalers = {}\n",
        "\n",
        "        for zone_id in zone_ids:\n",
        "            # Get zone-specific data\n",
        "            zone_data = train_df[train_df['zone_id'] == zone_id].sort_values('timestamp')\n",
        "\n",
        "            # Select features\n",
        "            features = ['hour', 'day_of_week', 'is_weekend', 'is_holiday', 'ride_requests']\n",
        "            weather_features = [col for col in zone_data.columns if col.startswith('weather_')]\n",
        "            features.extend(weather_features)\n",
        "\n",
        "            # Extract feature matrix\n",
        "            data = zone_data[features].values\n",
        "\n",
        "            # Scale the data\n",
        "            scaler = StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    def train_tcn_model(self, train_df, zone_ids, sequence_length=24):\n",
        "        \"\"\"\n",
        "        Train a Temporal Convolutional Network (TCN) model.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        train_df : pandas.DataFrame\n",
        "            Training data.\n",
        "        zone_ids : list\n",
        "            List of zone IDs to include in training.\n",
        "        sequence_length : int\n",
        "            Number of time steps to use for each sequence.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        model : Keras TCN model\n",
        "            Trained TCN model.\n",
        "        scalers : dict\n",
        "            Dictionary of scalers for each zone.\n",
        "        \"\"\"\n",
        "        from tensorflow.keras import Input, Model\n",
        "        from tensorflow.keras.layers import Dense, Dropout\n",
        "        from tcn import TCN\n",
        "\n",
        "        # Prepare data\n",
        "        X_sequences = []\n",
        "        y_values = []\n",
        "        scalers = {}\n",
        "\n",
        "        for zone_id in zone_ids:\n",
        "            # Get zone-specific data\n",
        "            zone_data = train_df[train_df['zone_id'] == zone_id].sort_values('timestamp')\n",
        "\n",
        "            # Select features\n",
        "            features = ['hour', 'day_of_week', 'is_weekend', 'is_holiday', 'ride_requests']\n",
        "            weather_features = [col for col in zone_data.columns if col.startswith('weather_')]\n",
        "            features.extend(weather_features)\n",
        "\n",
        "            # Extract feature matrix\n",
        "            data = zone_data[features].values\n",
        "\n",
        "            # Scale the data\n",
        "            scaler = StandardScaler()\n",
        "            scaled_data = scaler.fit_transform(data)\n",
        "            scalers[zone_id] = scaler\n",
        "\n",
        "            # Create sequences\n",
        "            for i in range(len(scaled_data) - sequence_length):\n",
        "                X_sequences.append(scaled_data[i:i+sequence_length, :])\n",
        "                y_values.append(scaled_data[i+sequence_length, -1])  # Predict ride_requests\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        X = np.array(X_sequences)\n",
        "        y = np.array(y_values)\n",
        "\n",
        "        # Build TCN model\n",
        "        inputs = Input(shape=(sequence_length, X.shape[2]))\n",
        "        tcn_layer = TCN(nb_filters=64, kernel_size=3, nb_stacks=2,\n",
        "                       dilations=[1, 2, 4, 8], return_sequences=False,\n",
        "                       activation='relu')(inputs)\n",
        "        dropout = Dropout(0.2)(tcn_layer)\n",
        "        outputs = Dense(1)(dropout)\n",
        "\n",
        "        model = Model(inputs, outputs)\n",
        "\n",
        "        # Compile the model\n",
        "        model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "        # Train the model\n",
        "        model.fit(X, y, epochs=30, batch_size=64, validation_split=0.2, verbose=1)\n",
        "\n",
        "        return model, scalers\n",
        "\n",
        "    def create_ensemble(self, base_models, meta_model=GradientBoostingRegressor()):\n",
        "        \"\"\"\n",
        "        Create an ensemble model using predictions from base models.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        base_models : dict\n",
        "            Dictionary of trained base models\n",
        "        meta_model : sklearn estimator\n",
        "            Meta-learner model (default: GradientBoostingRegressor)\n",
        "        \"\"\"\n",
        "        self.ensemble_meta_model = meta_model\n",
        "        self.base_models = base_models\n",
        "\n",
        "    def predict_ensemble(self, X):\n",
        "        \"\"\"\n",
        "        Make predictions using the ensemble model.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : pandas.DataFrame\n",
        "            Input features\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        predictions : numpy array\n",
        "            Ensemble predictions\n",
        "        \"\"\"\n",
        "        # Generate base model predictions\n",
        "        meta_features = []\n",
        "        for model_name, model in self.base_models.items():\n",
        "            if model_name == 'prophet':\n",
        "                preds = self._predict_prophet(X, model)\n",
        "            elif model_name == 'arima':\n",
        "                preds = self._predict_arima(X, model)\n",
        "            else:\n",
        "                preds = model.predict(X)\n",
        "            meta_features.append(preds)\n",
        "\n",
        "        # Stack predictions horizontally\n",
        "        meta_features = np.column_stack(meta_features)\n",
        "\n",
        "        # Make final prediction\n",
        "        return self.ensemble_meta_model.predict(meta_features)\n",
        "\n",
        "    def evaluate_model(self, model, test_df, preprocessor=None, model_type='ml'):\n",
        "        \"\"\"\n",
        "        Evaluate a trained model on test data.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        model : trained model\n",
        "            The model to evaluate\n",
        "        test_df : pandas.DataFrame\n",
        "            Test dataset\n",
        "        preprocessor : sklearn preprocessor\n",
        "            Feature preprocessor (required for ML models)\n",
        "        model_type : str\n",
        "            Type of model ('arima', 'prophet', 'lstm', 'tcn', 'ml')\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict\n",
        "            Dictionary of evaluation metrics\n",
        "        \"\"\"\n",
        "        X_test = test_df.drop(['ride_requests', 'timestamp', 'city', 'zone_id'], axis=1)\n",
        "        y_test = test_df['ride_requests']\n",
        "\n",
        "        if model_type in ['arima', 'prophet']:\n",
        "            # Time series models require zone-by-zone evaluation\n",
        "            predictions = []\n",
        "            for zone_id in test_df['zone_id'].unique():\n",
        "                zone_data = test_df[test_df['zone_id'] == zone_id]\n",
        "                if model_type == 'arima':\n",
        "                    pred = model[zone_id].predict(start=len(zone_data),\n",
        "                                                end=len(zone_data),\n",
        "                                                typ='levels')\n",
        "                elif model_type == 'prophet':\n",
        "                    future = model[zone_id].make_future_dataframe(\n",
        "                        periods=len(zone_data),\n",
        "                        freq='30min'\n",
        "                    )\n",
        "                    pred = model[zone_id].predict(future)['yhat'][-len(zone_data):]\n",
        "                predictions.extend(pred)\n",
        "        elif model_type in ['lstm', 'tcn']:\n",
        "            # Sequence models require sequence generation\n",
        "            predictions = self._predict_sequences(model, test_df, model_type)\n",
        "        else:\n",
        "            if preprocessor:\n",
        "                X_processed = preprocessor.transform(X_test)\n",
        "            predictions = model.predict(X_processed)\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics = {\n",
        "            'MAE': mean_absolute_error(y_test, predictions),\n",
        "            'RMSE': np.sqrt(mean_squared_error(y_test, predictions)),\n",
        "            'MAPE': mean_absolute_percentage_error(y_test, predictions)\n",
        "        }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def run_full_pipeline(self, cities=None, horizon='3h'):\n",
        "        \"\"\"\n",
        "        Execute complete forecasting pipeline.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        cities : list\n",
        "            List of cities to process\n",
        "        horizon : str\n",
        "            Forecast horizon (default: 3 hours)\n",
        "        \"\"\"\n",
        "        # Load and preprocess data\n",
        "        if self.ride_data is None:\n",
        "            self.generate_dummy_data()\n",
        "\n",
        "        processed_data = self.preprocess_data()\n",
        "\n",
        "        # Train models for each city\n",
        "        for city in cities or self.cities:\n",
        "            print(f\"\\nTraining models for {city}\")\n",
        "            city_data = processed_data[city]['train']\n",
        "\n",
        "            # Initialize model storage\n",
        "            self.models[city] = {\n",
        "                'arima': {},\n",
        "                'prophet': {},\n",
        "                'xgb': None,\n",
        "                'rf': None,\n",
        "                'lstm': None,\n",
        "                'tcn': None,\n",
        "                'ensemble': None\n",
        "            }\n",
        "\n",
        "            # Get unique zones\n",
        "            zones = city_data['zone_id'].unique()\n",
        "\n",
        "            # Train ARIMA models\n",
        "            print(\"Training ARIMA models...\")\n",
        "            for zone in zones:\n",
        "                self.models[city]['arima'][zone] = self.train_arima_model(city_data, zone)\n",
        "\n",
        "            # Train Prophet models\n",
        "            print(\"Training Prophet models...\")\n",
        "            for zone in zones:\n",
        "                self.models[city]['prophet'][zone] = self.train_prophet_model(city_data, zone)\n",
        "\n",
        "            # Train ML models\n",
        "            print(\"Training XGBoost...\")\n",
        "            preprocessor = self.build_feature_pipeline(city_data)\n",
        "            self.models[city]['xgb'] = self.train_xgboost_model(city_data, preprocessor)\n",
        "\n",
        "            print(\"Training Random Forest...\")\n",
        "            self.models[city]['rf'] = self.train_random_forest_model(city_data, preprocessor)\n",
        "\n",
        "            # Train DL models\n",
        "            print(\"Training LSTM...\")\n",
        "            self.models[city]['lstm'], lstm_scalers = self.train_lstm_model(city_data, zones)\n",
        "\n",
        "            print(\"Training TCN...\")\n",
        "            self.models[city]['tcn'], tcn_scalers = self.train_tcn_model(city_data, zones)\n",
        "\n",
        "            # Create ensemble\n",
        "            print(\"Creating ensemble...\")\n",
        "            base_models = {\n",
        "                'xgb': self.models[city]['xgb'],\n",
        "                'rf': self.models[city]['rf'],\n",
        "                'lstm': self.models[city]['lstm'],\n",
        "                'tcn': self.models[city]['tcn']\n",
        "            }\n",
        "            self.create_ensemble(base_models)\n",
        "\n",
        "            # Evaluate models\n",
        "            print(\"\\nEvaluating models...\")\n",
        "            test_df = processed_data[city]['test']\n",
        "\n",
        "            # Evaluate XGBoost\n",
        "            xgb_metrics = self.evaluate_model(\n",
        "                self.models[city]['xgb'],\n",
        "                test_df,\n",
        "                preprocessor\n",
        "            )\n",
        "\n",
        "            # Evaluate Ensemble\n",
        "            ensemble_metrics = self.evaluate_ensemble(test_df, preprocessor)\n",
        "\n",
        "            print(f\"\\n{city} Results:\")\n",
        "            print(f\"XGBoost - MAE: {xgb_metrics['MAE']:.2f}, MAPE: {xgb_metrics['MAPE']:.2f}%\")\n",
        "            print(f\"Ensemble - MAE: {ensemble_metrics['MAE']:.2f}, MAPE: {ensemble_metrics['MAPE']:.2f}%\")\n",
        "\n",
        "    def _predict_sequences(self, model, test_df, model_type):\n",
        "        \"\"\"Helper method for sequence model predictions\"\"\"\n",
        "        predictions = []\n",
        "        for zone_id in test_df['zone_id'].unique():\n",
        "            zone_data = test_df[test_df['zone_id'] == zone_id].sort_values('timestamp')\n",
        "            # Implement sequence prediction logic here\n",
        "            # ... (omitted for brevity)\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def evaluate_ensemble(self, test_df, preprocessor):\n",
        "        \"\"\"Evaluate ensemble model performance\"\"\"\n",
        "        X_test = test_df.drop(['ride_requests', 'timestamp', 'city', 'zone_id'], axis=1)\n",
        "        X_processed = preprocessor.transform(X_test)\n",
        "\n",
        "        # Get base model predictions\n",
        "        base_preds = []\n",
        "        for model in self.base_models.values():\n",
        "            if isinstance(model, (tf.keras.Model, Sequential)):\n",
        "                preds = model.predict(X_processed)\n",
        "            else:\n",
        "                preds = model.predict(X_processed)\n",
        "            base_preds.append(preds)\n",
        "\n",
        "        # Combine predictions\n",
        "        meta_features = np.column_stack(base_preds)\n",
        "        final_preds = self.ensemble_meta_model.predict(meta_features)\n",
        "\n",
        "        # Calculate metrics\n",
        "        return {\n",
        "            'MAE': mean_absolute_error(test_df['ride_requests'], final_preds),\n",
        "            'RMSE': np.sqrt(mean_squared_error(test_df['ride_requests'], final_preds)),\n",
        "            'MAPE': mean_absolute_percentage_error(test_df['ride_requests'], final_preds)\n",
        "        }\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "  forecast_system = OlaBikeForecast()\n",
        "\n",
        "    # Generate and use synthetic data\n",
        "  forecast_system.generate_dummy_data()\n",
        "\n",
        "    # Run complete pipeline for Bangalore\n",
        "  forecast_system.run_full_pipeline(cities=[\"Bangalore\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "UpwrMfzIiyEG",
        "outputId": "9715fb50-ff0e-4610-def4-14b36942fe3c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating synthetic data for demonstration...\n",
            "Synthetic data generation complete!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'OlaBikeForecast' object has no attribute 'run_full_pipeline'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-a55726a2d96f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;31m# Run complete pipeline for Bangalore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m   \u001b[0mforecast_system\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_full_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcities\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Bangalore\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'OlaBikeForecast' object has no attribute 'run_full_pipeline'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hsEoAlSaiyuJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}